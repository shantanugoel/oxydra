# Oxydra agent configuration.
# Copy to ./.oxydra/agent.toml and customize as needed.
#
# All values shown below are the defaults. Uncomment and change as needed.
# For full documentation, see docs/guidebook/02-configuration-system.md

config_version = "1.0.0"

# --- Provider selection ---
# The provider name must match an entry in [providers.registry.*].
# Supported provider types: "openai", "anthropic", "gemini", "openai_responses"
# Available models (from pinned catalog):
#   OpenAI:    gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, gpt-4o, gpt-4o-mini, o3-mini
#   Anthropic: claude-3-5-haiku-latest, claude-3-5-sonnet-latest
#   Google:    (populated by `catalog fetch`; set GEMINI_API_KEY)
[selection]
provider = "openai"
model    = "gpt-4o-mini"

# --- Specialist agents (optional) ---
# Sessions created with agent_name route to that agent's effective selection:
# - explicit [agents.<name>.selection] => use that provider/model
# - no [agents.<name>.selection] => inherit root [selection]
# - "default" always uses root [selection]
#
# Delegation (`delegate_to_agent`) uses the same override semantics, except
# agents without a selection inherit the caller's current provider/model.

# [agents.multimodal_looker]
# system_prompt = "You are a multimodal looker, skilled at gathering and summarizing information from given media"
# tools = ["file_read"]
# max_turns = 12
# 
# [agents.multimodal_looker.selection]
# provider = "gemini"
# model = "gemini-3-flash-preview"
# 
# [agents.imager]
# system_prompt = "You are a creative artist, skilled at generating and editing images based on user requests. You can create new images or modify existing ones to meet the user's needs."
# tools = []
# max_turns = 10
# 
# [agents.imager.selection]
# provider = "gemini"
# model = "gemini-3.1-flash-image-preview"

# [agents.researcher]
# system_prompt = "You are a research specialist focused on gathering and summarizing evidence."
# tools = ["web_search", "web_fetch", "file_read"]
# max_turns = 12
# max_cost = 1.0
#
# [agents.researcher.selection]
# provider = "anthropic"
# model = "claude-3-5-haiku-latest"
#
# [agents.coder]
# system_prompt = "You are a coding specialist focused on implementation and debugging."
# tools = ["file_read", "file_edit", "file_write", "shell_exec"]
# # No [agents.coder.selection] block: inherits caller/root selection.

# --- Runtime limits ---
[runtime]
turn_timeout_secs = 60   # Max seconds per LLM call
max_turns         = 25    # Max agentic loop iterations
# max_cost        = 1000.0  # Optional cumulative cost cap (provider-reported units)

# Context window management — triggers summarization when usage exceeds trigger_ratio.
[runtime.context_budget]
trigger_ratio              = 0.85    # Fraction of max context that triggers compaction
safety_buffer_tokens       = 1024    # Tokens reserved below the hard limit
fallback_max_context_tokens = 128000 # Used when the model catalog has no entry

# Conversation summarization — compresses older turns to stay within budget.
[runtime.summarization]
target_ratio = 0.5  # Target context usage after summarization (fraction of max)
min_turns    = 6    # Minimum turns before summarization is eligible

# --- Gateway (session + top-level turn controls) ---
# Top-level user turns queue fairly (FIFO) once concurrent capacity is reached.
# Subagent turns run under the parent turn and do not consume extra top-level permits.
[gateway]
max_sessions_per_user = 50
max_concurrent_turns_per_user = 10
session_idle_ttl_hours = 48

# --- Memory (libSQL-backed persistence) ---
[memory]
enabled = false
# remote_url = "libsql://your-database.turso.io"  # Optional Turso remote
# auth_token = "set-me-or-use-OXYDRA__MEMORY__AUTH_TOKEN"  # Required when remote_url is set

# Hybrid retrieval weights (must sum to 1.0).
[memory.retrieval]
top_k         = 8    # Number of results returned per query
vector_weight = 0.7  # Weight for vector (embedding) similarity
fts_weight    = 0.3  # Weight for full-text search scoring

# --- Retry / reliability ---
[reliability]
max_attempts    = 3     # Total attempts per LLM call (1 = no retries)
backoff_base_ms = 250   # Initial backoff delay
backoff_max_ms  = 2000  # Maximum backoff delay
jitter          = false # Add randomized jitter to backoff

# --- Provider registry, endpoints, and credentials ---
# API keys can also be set via environment variables:
#   OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY, or fallback API_KEY
#
# The base_url can point to any compatible endpoint (e.g. OpenRouter,
# Azure OpenAI, or a local proxy).

# --- Example: Google Gemini ---
# [providers.registry.gemini]
# provider_type = "gemini" # Configures the `type` of API to communicate upstream
# api_key_env = "GEMINI_API_KEY"

# --- Example: OpenAI Responses API (stateful session chaining) ---
# [providers.registry.openai-responses]
# provider_type = "openai_responses"
# api_key_env = "OPENAI_API_KEY"

# --- Example: OpenRouter (or other OpenAI-compatible proxy) ---
# Requires catalog.skip_catalog_validation = true for models not in the
# pinned catalog.
# [providers.registry.openrouter]
# provider_type = "openai"
# base_url = "https://openrouter.ai/api/v1"
# api_key_env = "OPENROUTER_API_KEY"
# catalog_provider = "openrouter"
# reasoning = true              # If the model supports reasoning/thinking
# max_context_tokens = 200000   # Override default 128K context
# max_output_tokens = 8192      # Override default 16K output

# --- Tool-specific configuration ---
# Structured alternative to OXYDRA_WEB_SEARCH_* environment variables.
# Explicit env vars always take precedence over values set here.

# [tools.web_search]
# provider = "duckduckgo"                     # "duckduckgo" (default), "google", or "searxng"
# base_url = "https://api.duckduckgo.com/"    # Override default base URL for the selected provider
# base_urls = "https://url1,https://url2"     # Comma-separated fallback base URLs
# api_key_env = "MY_GOOGLE_API_KEY"           # Env var name holding Google API key
# engine_id_env = "MY_GOOGLE_CX"             # Env var name holding Google engine/CX ID
# query_params = "key=value&key2=value2"      # Extra query parameters
# # SearxNG-specific:
# engines = "google,bing"                     # SearxNG search engines
# categories = "general"                      # SearxNG categories
# safesearch = 1                              # SearxNG safesearch level (0-2)
# # Self-hosted / local service access:
# # List hosts (with optional port) that are allowed to resolve to private or
# # loopback addresses. Required when pointing web_search or web_fetch at a
# # local service such as a self-hosted SearxNG instance.
# egress_allowlist = ["localhost:8888"]
# # Multiple entries:
# # egress_allowlist = ["localhost:8888", "192.168.1.50:9000"]

# --- Attachment save configuration ---
# Configure the attachment_save tool for persisting inbound user attachments.
# [tools.attachment_save]
# timeout_secs = 60   # Timeout in seconds for save operations (default: 60)

# --- Shell command allowlist ---
# Configure which commands the LLM is allowed to execute via shell_exec.
# By default, a built-in allowlist of ~30 common commands is used.

# [tools.shell]
# allow = ["npm", "npx", "curl", "wget", "jq", "make", "docker", "rg"]  # Add to defaults
# deny = ["rm"]                      # Remove from defaults
# replace_defaults = false           # If true, `allow` replaces the default list entirely
# allow_operators = false            # If true, allow shell operators (&&, ||, |, etc.)
# env_keys = ["NPM_TOKEN", "GH_TOKEN"]  # Env vars to forward into the shell container
# Note: API keys from the agent config are NOT forwarded to the shell.
# Use env_keys for shell-specific secrets. CLI --env entries prefixed
# with SHELL_ are also forwarded with the prefix stripped
# (e.g. --env SHELL_NPM_TOKEN=xxx becomes NPM_TOKEN in the shell).

# --- Scheduler ---
# Enable and configure the scheduler for automated recurring/one-off tasks.
# [scheduler]
# enabled = true                      # Enable scheduling (default: false)
# poll_interval_secs = 15             # How often to check for due tasks
# max_concurrent = 2                  # Max concurrent scheduled runs
# max_schedules_per_user = 50         # Per-user schedule limit
# max_turns = 10                      # Max turns per scheduled run (operator-only)
# max_cost = 0.50                     # Max cost per scheduled run (operator-only)
# max_run_history = 20                # Run history entries to retain
# min_interval_secs = 60              # Minimum interval (anti-abuse)
# default_timezone = "Asia/Kolkata"   # Default timezone for cron schedules
# auto_disable_after_failures = 5     # Auto-disable after N consecutive failures

# --- Catalog resolution and validation ---
[catalog]
skip_catalog_validation = false  # Set true to allow models not in the catalog
# pinned_url = "https://raw.githubusercontent.com/..."  # Override pinned snapshot URL
